{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.application import VGG16\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import Callbacks,Modelcheckpoint ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 256, 256\n",
    "image_size = [256,256]\n",
    "c=3\n",
    "class_name = [\"brain_glioma\",\"brain_menin\",\"brain_tumor\"]\n",
    "lr = 1e-4\n",
    "model_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/kaggle/input/multi-cancer/Multi Cancer/Brain Cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,split=0.1):\n",
    "    files = glob(os.path.join(path,\"*\",\"*\"))\n",
    "    split_rate = int(len(files) * split)\n",
    "    \n",
    "    train,valid = train_test_split(files,test_size=split_rate)\n",
    "    train,test = train_test_split(train,test_size=split_rate)\n",
    "\n",
    "    return train,valid,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files =load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image):\n",
    "    img = cv2.imread(image,cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img,(H.W))\n",
    "    img = img / 255.0\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    lable = image.split(\"/\")[-2]\n",
    "    class_idx = classes_name.index[lable]\n",
    "\n",
    "    return img,class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,class_idx = preprocess_data(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(class_idx,np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    images,labels = tf.numpy_function(preprocess_data,[path],[tf.float32,tf.int32])\n",
    "    labels = tf.one_hot(labels,3)\n",
    "    images.set_shape([256,256,3])\n",
    "    labels.set_shape(3)\n",
    "\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_datasets(images, batch_size=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images))\n",
    "    dataset = dataset.map(parse)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(8)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf_datasets(train)\n",
    "valid_ds = tf_datasets(valid)\n",
    "test_ds = tf_datasets(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds = tf_datasets(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in image_ds.take(1):\n",
    "    print(i.numpy.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPipeImg(img_arr):\n",
    "    fig,ax = plt.subplots(1,10,figsize=(10,10))\n",
    "    axes = ax.flatten()\n",
    "    for img, ax in zip(img_arr,axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,idl = next(iter(image_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(input_shape=image_size+[c], weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = layers.Dense(3,activation='softmax')(x)\n",
    "model = Model(inputs=model.input,outputs=last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback=[\n",
    "    Modelcheckpoint(model_path,verbose=1,save_best_only = True),\n",
    "    ReduceLROnPlateau(monitor = \"val_Loss\",patience=5,min_lr = 1e-5,factor=0.1,verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='',optimizer=Adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    valid_ds,\n",
    "    epochs = 20,\n",
    "    Callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plt_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_mark = np.arange(len(classes))\n",
    "    plt.xticks(tick_mark, classes, rotation=45)\n",
    "    plt.yticks(tick_mark, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.axis]\n",
    "        print(\"normalized confusion matrix\")\n",
    "\n",
    "    else:\n",
    "        print(\"confusion matrix without normalization\")\n",
    "\n",
    "   thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"predicted label\")\n",
    "    plt.ylabel(\"True label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_class(test_path):\n",
    "    names = []\n",
    "    for i in test_path:\n",
    "        name = i.split(\"/\")[-2]\n",
    "        name_idx = class_name.index(name)\n",
    "        names.append(name_idx)\n",
    "    names = np.array(names, dtype=np.int32)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_test_data_class(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=classes, y_pred=y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_confusion_matrix(cm=cm, classes=class_name, title='cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion matrix without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the model on a given dataset\n",
    "def evaluate_model(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = np.argmax(labels, axis=1)\n",
    "        y_true.extend(true_labels)\n",
    "        y_pred.extend(predicted_labels)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    return accuracy, f1_scores\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_accuracy, test_f1_scores = evaluate_model(model, test_ds)\n",
    "\n",
    "# Evaluate the model on the train dataset (optional)\n",
    "train_accuracy, train_f1_scores = evaluate_model(model, train_ds)\n",
    "\n",
    "# Print the results\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"F1-Score (Giloma):\", test_f1_scores[0])\n",
    "print(\"F1-Score (Menin):\", test_f1_scores[1])\n",
    "print(\"F1-Score (Tumor):\", test_f1_scores[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
