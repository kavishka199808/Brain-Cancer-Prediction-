{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from glob import glob\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import*\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "H, W = 224, 224\n",
    "channel = 3\n",
    "IMAGE_SHAPE = [224, 224]\n",
    "num_class = 3\n",
    "batch_size = 64\n",
    "class_names = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/kaggle/working//models/vgg_for_brain_tumor-v2.h5\"\n",
    "path = \"/kaggle/input/multi-cancer/Multi Cancer/Brain Cancer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder for save augmented images\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split=0.1):\n",
    "    images = shuffle(glob(os.path.join(path, \"*\", \"*.jpg\")))\n",
    "    split_rate = int(len(images) * split)\n",
    "    train, valid = train_test_split(images, test_size=split_rate, random_state=42)\n",
    "    train, test = train_test_split(train, test_size=split_rate, random_state=42)\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(path):\n",
    "    #decode the path\n",
    "    path = path.decode()\n",
    "    #read image\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    #resize the image\n",
    "    image = cv2.resize(image, [224, 224])\n",
    "    #scale the image\n",
    "    image = image / 255.0\n",
    "    #change the data type of image\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    #labeling the image\n",
    "    class_name = path.split(\"/\")[-2]\n",
    "    class_idx = class_names.index(class_name)\n",
    "    class_idx = np.array(class_idx, dtype=np.int32)\n",
    "\n",
    "    return image, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    image, labels = tf.numpy_function(process_image, [path], (tf.float32, tf.int32))\n",
    "    labels = tf.one_hot(labels, 3)\n",
    "    image.set_shape([224, 224, 3])\n",
    "    labels.set_shape(3)\n",
    "  \n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow dataset\n",
    "def tf_dataset(images, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images))\n",
    "    dataset = dataset.map(parse)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(8)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model | Inception-V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incp = InceptionV3(input_shape=IMAGE_SHAPE+[channel], weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in incp.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(incp.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = layers.Dense(num_class, activation='softmax' )(x)\n",
    "#declare the model\n",
    "model = Model(inputs=incp.input, outputs=prediction_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir(\"/kaggle/working/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Train:{len(train)} Valid:{len(valid)}, Test:{len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tf_dataset(train)\n",
    "valid_df = tf_dataset(valid)\n",
    "test_df = tf_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in train_df.take(1):\n",
    "    print(i.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_df,\n",
    "    validation_data=test_df,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#plot confusion matrix\n",
    "def plt_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_mark = np.arange(len(classes))\n",
    "    plt.xticks(tick_mark, classes, rotation=45)\n",
    "    plt.yticks(tick_mark, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.axis]\n",
    "        print(\"normalized confusion matrix\")\n",
    "\n",
    "    else:\n",
    "        print(\"confusion matrix without normalization\")\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"predicted label\")\n",
    "    plt.ylabel(\"True label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "prediction = model.predict(test_df, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for get labels of test set\n",
    "def get_test_data_class(test_path):\n",
    "    names = []\n",
    "    for i in test_path:\n",
    "        name = i.split(\"/\")[-2]\n",
    "        name_idx = class_names.index(name)\n",
    "        names.append(name_idx)\n",
    "    names = np.array(names, dtype=np.int32)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_test_data_class(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=classes, y_pred=y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_confusion_matrix(cm=cm, classes=class_names, title=\"confusion matrix\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define a function to evaluate the model on a given dataset\n",
    "def evaluate_model(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = np.argmax(labels, axis=1)\n",
    "        y_true.extend(true_labels)\n",
    "        y_pred.extend(predicted_labels)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    return accuracy, f1_scores\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_accuracy, test_f1_scores = evaluate_model(model, test_df)\n",
    "\n",
    "# Evaluate the model on the train dataset (optional)\n",
    "train_accuracy, train_f1_scores = evaluate_model(model, train_df)\n",
    "\n",
    "# Print the results\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"F1-Score (Giloma):\", test_f1_scores[0])\n",
    "print(\"F1-Score (Menin):\", test_f1_scores[1])\n",
    "print(\"F1-Score (Tumor):\", test_f1_scores[2])\n",
    "\n",
    "# If you want to print the results for the train dataset as well\n",
    "# print(\"Train Accuracy:\", train_accuracy)\n",
    "# print(\"Train F1-Score (Giloma):\", train_f1_scores[0])\n",
    "# print(\"Train F1-Score (Menin):\", train_f1_scores[1])\n",
    "# print(\"Train F1-Score (Tumor):\", train_f1_scores[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
